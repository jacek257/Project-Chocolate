{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import multiprocessing\n",
    "import fnmatch\n",
    "import analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set limit for number of processes that can be run at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "limit = cores - 5 if cores > 8 else 1\n",
    "processes = [None] * limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grab the processed BOLD data, O2 contrast, and CO2 contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "bold_dir = '/home/ke/Desktop/all_bold/'\n",
    "shifted_dir = '/home/ke/Desktop/shifted_export/'\n",
    "\n",
    "# all grab all the .txt files in the bold folder\n",
    "bold_files = [str(file) for file in os.listdir(bold_dir) if file.upper().endswith('EDITS.TXT')]\n",
    "bold_files.sort()\n",
    "print(len(bold_files))\n",
    "O2_files = [file for file in os.listdir(shifted_dir) if file.upper().endswith('_O2.TXT')]\n",
    "O2_files.sort()\n",
    "print(len(O2_files))\n",
    "CO2_files = [file for file in os.listdir(shifted_dir) if file.upper().endswith('_CO2.TXT')]\n",
    "CO2_files.sort()\n",
    "print(len(CO2_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEAT already exists for BR006_20171107\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR011_20171101\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR013_20171108\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR014_20171115\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR023_20180130\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR027_20180328\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR031_20180416\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR035_20180703\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR037_20181012\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR038_20181026\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR040_20181203\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR042_20181204\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for BR043_20190128\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for WH1242_20180530\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for WH1651_20180501\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for WH1686_20181010\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for WH1698_20181025\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for WH1701_20181206\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "FEAT already exists for WH1721_20190813\n",
      "Overwriting\n",
      "Starting FEAT\n",
      "Waiting for the remaining  FEAT to finish\n",
      "\bStarting O2 featquery for BR006_20171107\n",
      "Starting featquery for CO2\n",
      "Starting O2 featquery for BR011_20171101\n",
      "Starting featquery for CO2\n",
      "Starting O2 featquery for BR013_20171108\n",
      "Starting featquery for CO2\n",
      "Starting O2 featquery for BR014_20171115\n",
      "Starting featquery for CO2\n",
      "Starting O2 featquery for BR023_20180130\n",
      "Starting featquery for CO2\n",
      "Starting O2 featquery for BR027_20180328\n",
      "Starting featquery for CO2\n",
      "Starting O2 featquery for BR031_20180416\n",
      "Starting featquery for CO2\n",
      "Starting O2 featquery for BR035_20180703\n",
      "Starting featquery for CO2\n",
      "Starting O2 featquery for BR037_20181012\n",
      "Starting featquery for CO2\n",
      "Starting O2 featquery for BR038_20181026\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting featquery for CO2\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting O2 featquery for BR040_20181203\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting featquery for CO2\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting O2 featquery for BR042_20181204\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting featquery for CO2\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting O2 featquery for BR043_20190128\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting featquery for CO2\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting O2 featquery for WH1242_20180530\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting featquery for CO2\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting O2 featquery for WH1651_20180501\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting featquery for CO2\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting O2 featquery for WH1686_20181010\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting featquery for CO2\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting O2 featquery for WH1698_20181025\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting featquery for CO2\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting O2 featquery for WH1701_20181206\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting featquery for CO2\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting O2 featquery for WH1721_20190813\n",
      "There are 19  featquery currently running. Limit reached. Waiting for at least one to end.\n",
      "\bStarting featquery for CO2\n",
      "Waiting for the remaining  featquery to finish\n",
      "\b"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'warnings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-c71859a74c7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mcz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_output_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'cluster_zstat1.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Voxels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-log10(P)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Z-MAX'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'COPE-MEAN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mt_vol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcz1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVoxels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/ke/Desktop/feat/BR006_20171107.feat/cluster_zstat1.txt' does not exist: b'/home/ke/Desktop/feat/BR006_20171107.feat/cluster_zstat1.txt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-c71859a74c7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mwarnings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'warning'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No cluster_zstat1.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0madd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'warnings' is not defined"
     ]
    }
   ],
   "source": [
    "feat_dir = '/home/ke/Desktop/feat/'\n",
    "T1_dir = '/home/ke/Desktop/all_T1/'\n",
    "\n",
    "# load design template\n",
    "with open(feat_dir+'design_files/template', 'r') as template:\n",
    "    stringTemp = template.read()\n",
    "    for i in range(len(bold_files)):\n",
    "        bold_path = bold_dir+'/'+bold_files[i]\n",
    "        bold_df = pd.read_csv(bold_path, names=['Time', 'BOLD'])\n",
    "        tr = bold_df.Time[1]\n",
    "        identify = bold_files[i].split('_')\n",
    "        sub_id = identify[0]\n",
    "        date = identify[1]\n",
    "        p_id = sub_id+'_'+date\n",
    "        key = ''\n",
    "        verb = True\n",
    "        over = True\n",
    "        \n",
    "        output_dir = feat_dir+bold_files[i][:-4]\n",
    "        T1_file = [T1_dir+file for file in os.listdir(T1_dir) if fnmatch.fnmatch(file, sub_id+'_'+date+'*_T1.nii*')]\n",
    "        \n",
    "        if not T1_file:\n",
    "            T1_file = [T1_dir+file for file in os.listdir(T1_dir) if fnmatch.fnmatch(file, sub_id+'_FS_T1.nii*')]\n",
    "        \n",
    "        if not T1_file:\n",
    "            T1_file = ['/usr/local/fsl/data/standard/MNI152_T1_2mm_brain']\n",
    "        \n",
    "        T1_file = T1_file[0]\n",
    "#         print(T1_file)\n",
    "# #            output_dir = '/media/ke/8tb_part2/FSL_work/feat/both_shift/'+key+df.ID[i]+'_'+df.Date[i]\n",
    "        if os.path.exists(output_dir+'.feat'):\n",
    "            if verb:\n",
    "                print('FEAT already exists for', p_id)\n",
    "            if over:\n",
    "                if verb:\n",
    "                    print('Overwriting')\n",
    "                subprocess.run(['rm', '-rf', output_dir+'.feat'])\n",
    "            else:\n",
    "                continue\n",
    "        to_write = stringTemp[:]\n",
    "       # print(to_write)\n",
    "        to_write = to_write.replace(\"%%OUTPUT_DIR%%\",'\"'+output_dir+'\"')\n",
    "        to_write = to_write.replace(\"%%VOLUMES%%\",'\"'+str(len(bold_df))+'\"')\n",
    "        to_write = to_write.replace(\"%%TR%%\",'\"'+str(tr)+'\"')\n",
    "        to_write = to_write.replace(\"%%BOLD_FILE%%\",'\"'+bold_path+'\"')\n",
    "        to_write = to_write.replace(\"%%FS_T1%%\",'\"'+T1_file+'\"')\n",
    "        to_write = to_write.replace(\"%%O2_CONTRAST%%\",'\"'+shifted_dir+O2_files[i]+'\"')\n",
    "        to_write = to_write.replace(\"%%CO2_CONTRAST%%\",'\"'+shifted_dir+CO2_files[i]+'\"')\n",
    "\n",
    "        ds_path = feat_dir+'design_files/'+p_id+'.fsf'\n",
    "        with open(ds_path, 'w+') as outFile:\n",
    "            outFile.write(to_write)\n",
    "\n",
    "        index = analysis.parallel_processing().get_next_avail(processes, verb, limit, key, 'FEAT')\n",
    "\n",
    "        if verb:\n",
    "            print('Starting FEAT')\n",
    "        processes[index] = subprocess.Popen(['feat', ds_path])\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    analysis.parallel_processing().wait_remaining(processes, verb, key, 'FEAT')\n",
    "\n",
    "# run featquery\n",
    "for i in range(len(bold_files)):\n",
    "    identify = bold_files[i].split('_')\n",
    "    sub_id = identify[0]\n",
    "    date = identify[1]\n",
    "    p_id = sub_id+'_'+date\n",
    "    key = ''\n",
    "    verb = True\n",
    "    feat_output_dir = feat_dir+p_id+'.feat/'\n",
    "#        feat_output_dir = '/media/ke/8tb_part2/FSL_work/feat/both_shift/'+p_id+'.feat/'\n",
    "    over = True\n",
    "\n",
    "    O2_mask_dir_path = feat_output_dir+'cluster_mask_zstat1.nii.gz'\n",
    "    CO2_mask_dir_path = feat_output_dir+'cluster_mask_zstat2.nii.gz'\n",
    "\n",
    "    index = analysis.parallel_processing().get_next_avail(processes, verb, limit, key, 'featquery')\n",
    "\n",
    "    if os.path.exists(feat_output_dir+'fq_O2'):\n",
    "        if verb:\n",
    "            print('O2 featquery already exists for', p_id)\n",
    "        if over:\n",
    "            if verb:\n",
    "                print('Overwriting')\n",
    "            processes[index] = subprocess.Popen(['featquery', '1', feat_output_dir, '1', 'stats/cope1', 'fq_O2', '-p', '-s', O2_mask_dir_path])\n",
    "    else:\n",
    "        if verb:\n",
    "            print('Starting O2 featquery for', p_id)\n",
    "        processes[index] = subprocess.Popen(['featquery', '1', feat_output_dir, '1', 'stats/cope1', 'fq_O2', '-p', '-s', O2_mask_dir_path])\n",
    "\n",
    "    index = analysis.parallel_processing().get_next_avail(processes, verb, limit, key, 'featquery')\n",
    "\n",
    "    if os.path.exists(feat_output_dir+'fq_CO2'):\n",
    "        if verb:\n",
    "            print('CO2 featquery already exists for', p_id)\n",
    "        if over:\n",
    "            if verb:\n",
    "                print('Overwriting')\n",
    "            processes[index] = subprocess.Popen(['featquery', '1', feat_output_dir, '1', 'stats/cope2', 'fq_CO2', '-p', '-s', CO2_mask_dir_path])\n",
    "    else:\n",
    "        if verb:\n",
    "            print('Starting featquery for CO2')\n",
    "        processes[index] = subprocess.Popen(['featquery', '1', feat_output_dir, '1', 'stats/cope2', 'fq_CO2', '-p', '-s', CO2_mask_dir_path])\n",
    "\n",
    "\n",
    "analysis.parallel_processing().wait_remaining(processes, verb, key, 'featquery')\n",
    "\n",
    "# get the stats\n",
    "for i in range(len(bold_files)):\n",
    "    identify = bold_files[i].split('_')\n",
    "    sub_id = identify[0]\n",
    "    date = identify[1]\n",
    "    p_id = sub_id+'_'+date      \n",
    "    add = True\n",
    "    key = ''\n",
    "    verb = True\n",
    "    feat_output_dir = feat_dir+p_id+'.feat/'\n",
    "#        output_dir = '/media/ke/8tb_part2/FSL_work/feat/both_shift/'+key+df.ID[i]+'_'+df.Date[i]\n",
    "#     feat_output_dir = output_dir+'.feat/'\n",
    "    over = True\n",
    "\n",
    "    try:\n",
    "        cz1 = pd.read_csv(feat_output_dir+'cluster_zstat1.txt', sep='\\t', usecols=['Voxels', '-log10(P)', 'Z-MAX', 'COPE-MEAN'])\n",
    "        t_vol = cz1.Voxels.sum()\n",
    "\n",
    "        for j in range(len(cz1)):\n",
    "            cz1.iloc[j] = cz1.iloc[j] * cz1.iloc[j].Voxels/t_vol\n",
    "\n",
    "\n",
    "        z1 = { 'ID' : [p_id],\n",
    "               'Voxels': [t_vol],\n",
    "               '-log10(p)' : [cz1['-log10(P)'].sum()],\n",
    "               'COPE-MEAN' : [cz1['COPE-MEAN'].sum()]}\n",
    "        cz1_final = pd.DataFrame(z1)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        warnings['ID'].append(p_id)\n",
    "        warnings['warning'].append('No cluster_zstat1.txt')\n",
    "        add = False\n",
    "        if verb:\n",
    "            print('No cluster_zstat1.txt', p_id)\n",
    "\n",
    "    try:\n",
    "        cz2 = pd.read_csv(feat_output_dir+'cluster_zstat2.txt', sep='\\t', usecols=['Voxels', '-log10(P)', 'Z-MAX', 'COPE-MEAN'])\n",
    "        t_vol = cz2.Voxels.sum()\n",
    "\n",
    "        for j in range(len(cz2)):\n",
    "            cz2.iloc[j] = cz2.iloc[j] * cz2.iloc[j].Voxels/t_vol\n",
    "\n",
    "\n",
    "        z2 = { 'ID' : [p_id],\n",
    "               'Voxels': [t_vol],\n",
    "               '-log10(p)' : [cz2['-log10(P)'].sum()],\n",
    "               'COPE-MEAN' : [cz2['COPE-MEAN'].sum()]}\n",
    "        cz2_final = pd.DataFrame(z2)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        warnings['ID'].append(p_id)\n",
    "        warnings['warning'].append('No cluster_zstat2.txt')\n",
    "        add = False\n",
    "        if verb:\n",
    "            print('No cluster_zstat2.txt', p_id)\n",
    "\n",
    "    build = cz1_final.merge(cz2_final, on=['ID'], suffixes=('_O2', '_CO2'))\n",
    "\n",
    "    O2_mask_dir_path = feat_output_dir+'cluster_mask_zstat1.nii.gz'\n",
    "    CO2_mask_dir_path = feat_output_dir+'cluster_mask_zstat2.nii.gz'\n",
    "\n",
    "    O2 = feat_output_dir+'fq_O2/'\n",
    "    try:\n",
    "        fq1 = pd.read_csv(O2+'report.txt', sep='\\t| ', header=None, usecols=[5], engine='python')\n",
    "        fq1 = fq1.rename(columns={5 : 'fq_mean'})\n",
    "        fq1['ID'] = p_id\n",
    "        fq1 = fq1[['ID', 'fq_mean']]\n",
    "        build = build.merge(fq1, on=['ID'], suffixes=('_O2', '_CO2'))\n",
    "    except FileNotFoundError:\n",
    "        warnings['ID'].append(p_id)\n",
    "        warnings['warning'].append('No O2 activation found')\n",
    "        add = False\n",
    "        if verb:\n",
    "            print('No O2 activation found for', p_id, 'O2')\n",
    "\n",
    "\n",
    "    CO2 = feat_output_dir+'fq_CO2/'\n",
    "    try:\n",
    "        fq2 = pd.read_csv(CO2+'report.txt', sep='\\t| ', header=None, usecols=[5], engine='python')\n",
    "        fq2 = fq2.rename(columns={5 : 'fq_mean'})\n",
    "        fq2['ID'] = p_id\n",
    "        fq2 = fq2[['ID', 'fq_mean']]\n",
    "        build = build.merge(fq2, on=['ID'], suffixes=('_O2', '_CO2'))\n",
    "    except FileNotFoundError:\n",
    "        warnings['ID'].append(p_id)\n",
    "        warnings['warning'].append('No CO2 activation found')\n",
    "        add = False\n",
    "        if verb:\n",
    "            print('No CO2 activation found for', p_id, 'CO2')\n",
    "\n",
    "#     build['O2_shift'] = df.O2_shift[i]\n",
    "#     build['CO2_shift'] = df.CO2_shift[i]\n",
    "#     build['O2_coeff'] = df.coeffs[i][0]\n",
    "#     build['CO2_coeff'] = df.coeffs[i][1]\n",
    "#     build['r'] = df.r[i]\n",
    "#     build['p_value'] = df.p_value[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    if verb:\n",
    "#        print('\\n\\nStarting to run feat')\n",
    "#    #run Feat\n",
    "#    #check for (and make) feat directory\n",
    "#    if not os.path.exists(feat_dir):\n",
    "#        os.mkdir(feat_dir)\n",
    "#    \n",
    "#    #make design file directory\n",
    "#    if not os.path.exists(feat_dir+'design_files/'):\n",
    "#        os.mkdir(feat_dir+'design_files/')\n",
    "#    \n",
    "#    # load design template\n",
    "#    with open(feat_dir+'design_files/template', 'r') as template:\n",
    "#        stringTemp = template.read()\n",
    "#        for i in range(len(df)):\n",
    "#            output_dir = feat_dir+key+df.ID[i]+'_'+df.Date[i]\n",
    "##            output_dir = '/media/ke/8tb_part2/FSL_work/feat/both_shift/'+key+df.ID[i]+'_'+df.Date[i]\n",
    "#            if os.path.exists(output_dir+'.feat'):\n",
    "#                if verb:\n",
    "#                    print('FEAT already exists for', key+df.ID[i]+'_'+df.Date[i])\n",
    "#                if over:\n",
    "#                    if verb:\n",
    "#                        print('Overwriting')\n",
    "#                    subprocess.run(['rm', '-rf', output_dir+'.feat'])\n",
    "#                else:\n",
    "#                    continue\n",
    "#            to_write = stringTemp[:]\n",
    "#            # print(to_write)\n",
    "#            to_write = to_write.replace(\"%%OUTPUT_DIR%%\",'\"'+output_dir+'\"')\n",
    "#            to_write = to_write.replace(\"%%VOLUMES%%\",'\"'+str(df.Volumes[i])+'\"')\n",
    "#            to_write = to_write.replace(\"%%TR%%\",'\"'+str(df.eff_TR[i])+'\"')\n",
    "#            to_write = to_write.replace(\"%%BOLD_FILE%%\",'\"'+df.BOLD_path[i]+'\"')\n",
    "#            to_write = to_write.replace(\"%%FS_T1%%\",'\"'+df.T1_path[i]+'\"')\n",
    "#            to_write = to_write.replace(\"%%O2_CONTRAST%%\",'\"'+df.ETO2[i]+'\"')\n",
    "#            to_write = to_write.replace(\"%%CO2_CONTRAST%%\",'\"'+df.ETCO2[i]+'\"')\n",
    "#    \n",
    "#            ds_path = feat_dir+'design_files/'+key+df.ID[i]+'_'+df.Date[i]+'.fsf'\n",
    "#            with open(ds_path, 'w+') as outFile:\n",
    "#                outFile.write(to_write)\n",
    "#                        \n",
    "#            index = analysis.parallel_processing().get_next_avail(processes, verb, limit, key, 'FEAT')\n",
    "#            \n",
    "#            if verb:\n",
    "#                print('Starting FEAT')\n",
    "#            processes[index] = subprocess.Popen(['feat', ds_path])\n",
    "#            time.sleep(0.5)\n",
    "#        \n",
    "#        analysis.parallel_processing().wait_remaining(processes, verb, key, 'FEAT')\n",
    "#        \n",
    "#    # run featquery\n",
    "#    for i in range(len(df)):\n",
    "#        p_id = key+df.ID[i]+'_'+df.Date[i]\n",
    "#        feat_output_dir = feat_dir+p_id+'.feat/'\n",
    "##        feat_output_dir = '/media/ke/8tb_part2/FSL_work/feat/both_shift/'+p_id+'.feat/'\n",
    "#        \n",
    "#        O2_mask_dir_path = feat_output_dir+'cluster_mask_zstat1.nii.gz'\n",
    "#        CO2_mask_dir_path = feat_output_dir+'cluster_mask_zstat2.nii.gz'\n",
    "#                    \n",
    "#        index = analysis.parallel_processing().get_next_avail(processes, verb, limit, key, 'featquery')\n",
    "#        \n",
    "#        if os.path.exists(feat_output_dir+'fq_O2'):\n",
    "#            if verb:\n",
    "#                print('O2 featquery already exists for', p_id)\n",
    "#            if over:\n",
    "#                if verb:\n",
    "#                    print('Overwriting')\n",
    "#                processes[index] = subprocess.Popen(['featquery', '1', feat_output_dir, '1', 'stats/cope1', 'fq_O2', '-p', '-s', O2_mask_dir_path])\n",
    "#        else:\n",
    "#            if verb:\n",
    "#                print('Starting O2 featquery for', p_id)\n",
    "#            processes[index] = subprocess.Popen(['featquery', '1', feat_output_dir, '1', 'stats/cope1', 'fq_O2', '-p', '-s', O2_mask_dir_path])\n",
    "#        \n",
    "#        index = analysis.parallel_processing().get_next_avail(processes, verb, limit, key, 'featquery')\n",
    "#        \n",
    "#        if os.path.exists(feat_output_dir+'fq_CO2'):\n",
    "#            if verb:\n",
    "#                print('CO2 featquery already exists for', p_id)\n",
    "#            if over:\n",
    "#                if verb:\n",
    "#                    print('Overwriting')\n",
    "#                processes[index] = subprocess.Popen(['featquery', '1', feat_output_dir, '1', 'stats/cope2', 'fq_CO2', '-p', '-s', CO2_mask_dir_path])\n",
    "#        else:\n",
    "#            if verb:\n",
    "#                print('Starting featquery for CO2')\n",
    "#            processes[index] = subprocess.Popen(['featquery', '1', feat_output_dir, '1', 'stats/cope2', 'fq_CO2', '-p', '-s', CO2_mask_dir_path])\n",
    "#    \n",
    "#\n",
    "#    analysis.parallel_processing().wait_remaining(processes, verb, key, 'featquery')\n",
    "        \n",
    "#     # get the stats\n",
    "#     for i in range(len(df)):        \n",
    "#         add = True\n",
    "        \n",
    "#        output_dir = feat_dir+key+df.ID[i]+'_'+df.Date[i]\n",
    "##        output_dir = '/media/ke/8tb_part2/FSL_work/feat/both_shift/'+key+df.ID[i]+'_'+df.Date[i]\n",
    "#        feat_output_dir = output_dir+'.feat/'\n",
    "#        \n",
    "#        try:\n",
    "#            cz1 = pd.read_csv(feat_output_dir+'cluster_zstat1.txt', sep='\\t', usecols=['Voxels', '-log10(P)', 'Z-MAX', 'COPE-MEAN'])\n",
    "#            t_vol = cz1.Voxels.sum()\n",
    "#            \n",
    "#            for j in range(len(cz1)):\n",
    "#                cz1.iloc[j] = cz1.iloc[j] * cz1.iloc[j].Voxels/t_vol\n",
    "#            \n",
    "#\n",
    "#            z1 = { 'ID' : [df.ID[i]+'_'+df.Date[i]],\n",
    "#                   'type' : [key],\n",
    "#                   'Voxels': [t_vol],\n",
    "#                   '-log10(p)' : [cz1['-log10(P)'].sum()],\n",
    "#                   'COPE-MEAN' : [cz1['COPE-MEAN'].sum()]}\n",
    "#            cz1_final = pd.DataFrame(z1)\n",
    "#        \n",
    "#        except FileNotFoundError:\n",
    "#            warnings['ID'].append(df.ID[i] + '_' + df.Date[i])\n",
    "#            warnings['warning'].append('No cluster_zstat1.txt')\n",
    "#            add = False\n",
    "#            if verb:\n",
    "#                print('No cluster_zstat1.txt')\n",
    "#        \n",
    "#        try:\n",
    "#            cz2 = pd.read_csv(feat_output_dir+'cluster_zstat2.txt', sep='\\t', usecols=['Voxels', '-log10(P)', 'Z-MAX', 'COPE-MEAN'])\n",
    "#            t_vol = cz2.Voxels.sum()\n",
    "#            \n",
    "#            for j in range(len(cz2)):\n",
    "#                cz2.iloc[j] = cz2.iloc[j] * cz2.iloc[j].Voxels/t_vol\n",
    "#            \n",
    "#\n",
    "#            z2 = { 'ID' : [df.ID[i]+'_'+df.Date[i]],\n",
    "#                   'type' : [key],\n",
    "#                   'Voxels': [t_vol],\n",
    "#                   '-log10(p)' : [cz2['-log10(P)'].sum()],\n",
    "#                   'COPE-MEAN' : [cz2['COPE-MEAN'].sum()]}\n",
    "#            cz2_final = pd.DataFrame(z2)\n",
    "#        \n",
    "#        except FileNotFoundError:\n",
    "#            warnings['ID'].append(df.ID[i] + '_' + df.Date[i])\n",
    "#            warnings['warning'].append('No cluster_zstat2.txt')\n",
    "#            add = False\n",
    "#            if verb:\n",
    "#                print('No cluster_zstat2.txt', df.ID[i], '_', df.Date[i])\n",
    "#        \n",
    "#        build = cz1_final.merge(cz2_final, on=['ID', 'type'], suffixes=('_O2', '_CO2'))\n",
    "#        \n",
    "#        O2_mask_dir_path = feat_output_dir+'cluster_mask_zstat1.nii.gz'\n",
    "#        CO2_mask_dir_path = feat_output_dir+'cluster_mask_zstat2.nii.gz'\n",
    "#            \n",
    "#        O2 = feat_output_dir+'fq_O2/'\n",
    "#        try:\n",
    "#            fq1 = pd.read_csv(O2+'report.txt', sep='\\t| ', header=None, usecols=[5], engine='python')\n",
    "#            fq1 = fq1.rename(columns={5 : 'fq_mean'})\n",
    "#            fq1['ID'] = df.ID[i]+'_'+df.Date[i]\n",
    "#            fq1['type'] = key\n",
    "#            fq1 = fq1[['ID', 'type', 'fq_mean']]\n",
    "#            build = build.merge(fq1, on=['ID', 'type'], suffixes=('_O2', '_CO2'))\n",
    "#        except FileNotFoundError:\n",
    "#            warnings['ID'].append(df.ID[i] + '_' + df.Date[i])\n",
    "#            warnings['warning'].append('No O2 activation found')\n",
    "#            add = False\n",
    "#            if verb:\n",
    "#                print('No O2 activation found for', df.ID[i] + '_' + df.Date[i], 'O2')\n",
    "#        \n",
    "#            \n",
    "#        CO2 = feat_output_dir+'fq_CO2/'\n",
    "#        try:\n",
    "#            fq2 = pd.read_csv(CO2+'report.txt', sep='\\t| ', header=None, usecols=[5], engine='python')\n",
    "#            fq2 = fq2.rename(columns={5 : 'fq_mean'})\n",
    "#            fq2['ID'] = df.ID[i]+'_'+df.Date[i]\n",
    "#            fq2['type'] = key\n",
    "#            fq2 = fq2[['ID', 'type', 'fq_mean']]\n",
    "#            build = build.merge(fq2, on=['ID', 'type'], suffixes=('_O2', '_CO2'))\n",
    "#        except FileNotFoundError:\n",
    "#            warnings['ID'].append(df.ID[i] + '_' + df.Date[i])\n",
    "#            warnings['warning'].append('No CO2 activation found')\n",
    "#            add = False\n",
    "#            if verb:\n",
    "#                print('No CO2 activation found for', df.ID[i] + '_' + df.Date[i], 'O2')\n",
    "#        \n",
    "#        build['O2_shift'] = df.O2_shift[i]\n",
    "#        build['CO2_shift'] = df.CO2_shift[i]\n",
    "#        build['O2_coeff'] = df.coeffs[i][0]\n",
    "#        build['CO2_coeff'] = df.coeffs[i][1]\n",
    "#        build['r'] = df.r[i]\n",
    "#        build['p_value'] = df.p_value[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
